{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "556b81cf",
   "metadata": {},
   "source": [
    "# Standard Reservoir Computer (paper version)\n",
    "\n",
    "### hyperparameters optimization\n",
    "\n",
    "### ( datasets by $\\rho \\in A =$ list of values  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae665baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import math     as math\n",
    "import numpy    as np\n",
    "import networkx as nx\n",
    "import random   as random\n",
    "import pandas   as pd\n",
    "import time     as time\n",
    "\n",
    "#######################################################################\n",
    "# E N V I R O N M E N T   S E T   U P\n",
    "#######################################################################\n",
    "#---------------------------------------------------------------------#\n",
    "# To compute elapsed time\n",
    "#---------------------------------------------------------------------#\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5bb9f7",
   "metadata": {},
   "source": [
    "## variables description\n",
    "    D       :(int  ) Input data dimension      \n",
    "    N       :(int  ) Reservoir dimension (degrees of freedome of the reservoir)       \n",
    "    rhoSR   :(float) Spectral Radius of A      \n",
    "    rhoA    :(float) Density of A              \n",
    "    alpha   :(float) Leak (or leakage) rate in (0,1]                 \n",
    "    sigma   :(float) Strength of input signal            \n",
    "    sigmab  :(float) Strength of input bias               \n",
    "    beta    :(float) Tichonov-Miller regularization parameter\n",
    "    \n",
    "    washout :(int)   During training, skipped transitory timesteps in W_out calculation\n",
    "    spinup  :(int)   Time (n. of timesteps) it takes for a trained RC to converge from its initial condition\n",
    "                     onto the synchronization manifold to which it is driven by the input data\n",
    "    normtime:(int)   Range to skip some QR factorisations and speed up the calculations  \n",
    "            \n",
    "    r       :(float) Reservoir state\n",
    "    W_in    :(float) Input matrix              \n",
    "    A       :(float) Reservoir ajacency matrix \n",
    "    b       :(float) bias vector               \n",
    "    W_out   :(float) Output matrix  \n",
    "    endtr_r :(float) Last reservoir state in training\n",
    "    \n",
    "    R       :(float) Matrix containing r(t) for all t in training dataset\n",
    "    U       :(float) Matrix containing u(t) for all t in training dataset\n",
    "    u       :(float) Time series at time t\n",
    "    v       :(float) Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "129942da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReservoirComputer class declaration\n",
    "class ReservoirComputer:\n",
    "    def __init__(self, D, N, rhoSR, rhoA, alpha, sigma, sigmab, beta):\n",
    "        self.r      = np.zeros(N)\n",
    "        self.W_in   = get_random_matrix(N, D, xa=-sigma, xb=sigma, nonzero=False)\n",
    "        self.A      = generate_adjacency_matrix(N, rhoSR, rhoA)\n",
    "        self.b      = sigmab * np.ones(N)\n",
    "        self.W_out  = np.zeros((D,N))\n",
    "        self.endtr_r= np.zeros(N)\n",
    "\n",
    "    def rc_update_rule(self, y):\n",
    "        # driven     mode: y = u(t)         ;r(t+1) = F^d_r (r(t),y)\n",
    "        # autonomous mode: y = W_out . r(t) ;r(t+1) = F^a_r (r(t),y)\n",
    "        g      = np.dot(self.A, self.r) + np.dot(self.W_in, y) + self.b\n",
    "        self.r = alpha * np.tanh(g) + (1 - alpha) * self.r\n",
    "\n",
    "    def update_v(self):\n",
    "        return np.dot(self.W_out, self.r)\n",
    "\n",
    "    def train(self, U, washout):\n",
    "        steps = U.shape[0]\n",
    "        R     = np.zeros((N, steps))\n",
    "        for i in range(steps):\n",
    "            R[:, i] = self.r\n",
    "            u       = U[i]\n",
    "            self.rc_update_rule(u)\n",
    "        self.W_out = linear_regression(R[:,washout:], U[washout:], beta)\n",
    "        self.endtr_r = self.r # save last training r state\n",
    "\n",
    "    def spinup(self, U, steps):\n",
    "        self.r = self.endtr_r # reset reservoir state\n",
    "        if steps > 0:\n",
    "            for i in range(steps):\n",
    "                u = U[i]\n",
    "                self.rc_update_rule(u) # driven mode\n",
    "    \n",
    "    def predict(self, steps):\n",
    "        prediction = np.zeros((steps, D))\n",
    "        for i in range(steps):\n",
    "            v             = self.update_v()\n",
    "            prediction[i] = v\n",
    "            self.rc_update_rule(v)\n",
    "        return prediction\n",
    "\n",
    "    def rc_lyapunov_exponents(self, steps, dt, normtime):\n",
    "        save_r = self.r # save r state\n",
    "        self.r = self.endtr_r # reset r state\n",
    "        lyap   = np.zeros((N,steps))\n",
    "        M      = np.eye(N)\n",
    "        W      = self.A + np.dot(self.W_in,self.W_out)\n",
    "        j      = -1\n",
    "        for i in range(steps):\n",
    "            v     = self.update_v()\n",
    "            self.rc_update_rule(v) # update r\n",
    "            #\n",
    "            g     = np.dot(W, self.r) + self.b\n",
    "            DF    = alpha * np.dot(np.diag(1 - np.tanh(g)**2),W) \\\n",
    "                    + (1 - alpha) * np.eye(N)\n",
    "            Mn    = np.dot(DF,M)\n",
    "            if (i % normtime == 0):\n",
    "                Q,Rii = np.linalg.qr(Mn)\n",
    "                j     += 1\n",
    "                lyap[:,j] = np.log(abs(np.diag(Rii)))\n",
    "                M = Q\n",
    "        L = np.sum(lyap,1) / ((j+1)*dt)    \n",
    "        self.r = save_r # retrieve saved r_state\n",
    "        return L\n",
    "\n",
    "    def rc_conditional_lyapunov_exponents(self, U, dt, normtime):\n",
    "        save_r = self.r # save r state\n",
    "        self.r = np.zeros(N) # reset r state\n",
    "        steps  = U.shape[0]\n",
    "        lyap   = np.zeros((N,steps))\n",
    "        M      = np.eye(N)\n",
    "        j      = -1\n",
    "        for i in range(steps):\n",
    "            u       = U[i]\n",
    "            self.rc_update_rule(u) # update r\n",
    "            #\n",
    "            g  = np.dot(self.A, self.r) + np.dot(self.W_in, u) + self.b\n",
    "            DF = alpha * np.dot(np.diag(1 - np.tanh(g)**2),self.A) \\\n",
    "                 + (1 - alpha) * np.eye(N)\n",
    "            Mn = np.dot(DF,M)\n",
    "            if (i % normtime == 0):\n",
    "                Q,Rii = np.linalg.qr(Mn)\n",
    "                j     += 1\n",
    "                lyap[:,j] = np.log(abs(np.diag(Rii)))\n",
    "                M = Q\n",
    "        CL = np.sum(lyap,1) / ((j+1)*dt)\n",
    "        self.r = save_r # retrieve saved r_state\n",
    "        return CL\n",
    "    \n",
    "# Helper functions\n",
    "def generate_adjacency_matrix(N, rhoSR, rhoA):\n",
    "    # Erdos-Reyni network\n",
    "    graph = nx.gnp_random_graph(N, rhoA)\n",
    "    adj   = nx.to_numpy_array(graph)\n",
    "    # Ensure random_array is of the same shape as the graph adjacency matrix\n",
    "    random_array = get_random_matrix(N, N, xa =-0.5, xb=0.5, nonzero=True)\n",
    "    # Multiply graph adjacency matrix with random values\n",
    "    rescaled     = adj * random_array\n",
    "    return scale_matrix(rescaled, rhoSR)\n",
    "\n",
    "def get_random_matrix(nrow, ncol, xa, xb, nonzero):\n",
    "    B = np.zeros((nrow,ncol))\n",
    "    for i in range(nrow):\n",
    "        for j in range(ncol):\n",
    "            if nonzero:\n",
    "                while B[i,j] == 0:\n",
    "                    B[i,j] = xa + (xb - xa) * np.random.rand()\n",
    "            else:\n",
    "                B[i,j] = xa + (xb - xa) * np.random.rand()\n",
    "    return B\n",
    "\n",
    "def scale_matrix(A, rhoSR):\n",
    "    eigenvalues = np.linalg.eigvals(A)    # compute eigenvlaues\n",
    "    sr = np.max(np.absolute(eigenvalues)) # compute spectral radius of A\n",
    "    if sr > 0:\n",
    "        A = A * rhoSR / sr                # rescaling matrix if A non zero\n",
    "    return A\n",
    "\n",
    "def linear_regression(R, U, beta=.0001): \n",
    "    Rt = np.transpose(R)\n",
    "    inverse_part = np.linalg.inv(np.dot(R, Rt) + beta * np.identity(R.shape[0]))\n",
    "    return np.dot(np.dot(U.T, Rt), inverse_part)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f1b2277",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# O P T I M I Z A T I O N   L O S S   F U N C T I O N \n",
    "#######################################################################\n",
    "# spinup_list = list of spinups values for forecasting \n",
    "def loss_macro(RC,test_data,spinup_list,forecast_steps):\n",
    "    loss_value = 0.\n",
    "    for i in range(len(spinup_list)):\n",
    "        spinup_steps = spinup_list[i]\n",
    "        RC.spinup(test_data, spinup_steps)\n",
    "        #\n",
    "        pred_data = RC.predict(forecast_steps)\n",
    "        #\n",
    "        for j in range(forecast_steps):\n",
    "            loss_value += np.linalg.norm(test_data[spinup_steps+j] - pred_data[j])**2 \\\n",
    "                        * np.exp(- (j+1) / (forecast_steps) )\n",
    "    return loss_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42a8094c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HP list length = 36\n"
     ]
    }
   ],
   "source": [
    "#######################################################################\n",
    "# H Y P E R P A R A M E T E R S   L I S T \n",
    "#######################################################################\n",
    "N_list      = [200]\n",
    "rhoSR_list  = [0.2, 0.5, 0.8]\n",
    "rhoA_list   = [0.02]\n",
    "alpha_list  = [0.4, 0.6, 0.8]\n",
    "sigma_list  = [0.084]\n",
    "sigmab_list = [1.1, 1.3, 1.5, 1.7]\n",
    "beta_list   = [8.5e-8]\n",
    "\n",
    "cart_prod = [(a,b,c,d,e,f,g)\n",
    "             for a in N_list       \n",
    "             for b in rhoSR_list   \n",
    "             for c in rhoA_list    \n",
    "             for d in alpha_list   \n",
    "             for e in sigma_list   \n",
    "             for f in sigmab_list  \n",
    "             for g in beta_list   ]\n",
    "\n",
    "len_vec = len(cart_prod)\n",
    "vec_loss_macro = np.zeros(len_vec)\n",
    "\n",
    "print(\"HP list length =\",len_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae59a5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n"
     ]
    }
   ],
   "source": [
    "#######################################################################\n",
    "# L I S T   O F   R H O   V A L U E S\n",
    "#######################################################################\n",
    "rho_list  = np.arange(166, 167.01, .01)\n",
    "rho_list  = np.round(rho_list,2)\n",
    "\n",
    "rho_list  = np.sort(rho_list)\n",
    "\n",
    "len_rho_list = len(rho_list)\n",
    "\n",
    "print(len_rho_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33351fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166.0\n",
      "166.01\n",
      "166.02\n",
      "166.03\n",
      "166.04\n",
      "166.05\n",
      "166.06\n",
      "166.07\n",
      "166.08\n",
      "166.09\n",
      "166.1\n",
      "166.11\n",
      "166.12\n",
      "166.13\n",
      "166.14\n",
      "166.15\n",
      "166.16\n",
      "166.17\n",
      "166.18\n",
      "166.19\n",
      "166.2\n",
      "166.21\n",
      "166.22\n",
      "166.23\n",
      "166.24\n",
      "166.25\n",
      "166.26\n",
      "166.27\n",
      "166.28\n",
      "166.29\n",
      "166.3\n",
      "166.31\n",
      "166.32\n",
      "166.33\n",
      "166.34\n",
      "166.35\n",
      "166.36\n",
      "166.37\n",
      "166.38\n",
      "166.39\n",
      "166.4\n",
      "166.41\n",
      "166.42\n",
      "166.43\n",
      "166.44\n",
      "166.45\n",
      "166.46\n",
      "166.47\n",
      "166.48\n",
      "166.49\n",
      "166.5\n",
      "166.51\n",
      "166.52\n",
      "166.53\n",
      "166.54\n",
      "166.55\n",
      "166.56\n",
      "166.57\n",
      "166.58\n",
      "166.59\n",
      "166.6\n",
      "166.61\n",
      "166.62\n",
      "166.63\n",
      "166.64\n",
      "166.65\n",
      "166.66\n",
      "166.67\n",
      "166.68\n",
      "166.69\n",
      "166.7\n",
      "166.71\n",
      "166.72\n",
      "166.73\n",
      "166.74\n",
      "166.75\n",
      "166.76\n",
      "166.77\n",
      "166.78\n",
      "166.79\n",
      "166.8\n",
      "166.81\n",
      "166.82\n",
      "166.83\n",
      "166.84\n",
      "166.85\n",
      "166.86\n",
      "166.87\n",
      "166.88\n",
      "166.89\n",
      "166.9\n",
      "166.91\n",
      "166.92\n",
      "166.93\n",
      "166.94\n",
      "166.95\n",
      "166.96\n",
      "166.97\n",
      "166.98\n",
      "166.99\n",
      "167.0\n"
     ]
    }
   ],
   "source": [
    "for s in range(len_rho_list):\n",
    "    print(rho_list[s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b113a7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====     0) : Lorenz rho = 166.000\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     107241.63283201172\n",
      "=====     1) : Lorenz rho = 166.010\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.8\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     236612.14496367288\n",
      "=====     2) : Lorenz rho = 166.020\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     2163408.113805578\n",
      "=====     3) : Lorenz rho = 166.030\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     1853602290.2904356\n",
      "=====     4) : Lorenz rho = 166.040\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     4294603.113177792\n",
      "=====     5) : Lorenz rho = 166.050\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     421742.8086235119\n",
      "=====     6) : Lorenz rho = 166.060\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     1297185.6522130105\n",
      "=====     7) : Lorenz rho = 166.070\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     5131529254.402785\n",
      "=====     8) : Lorenz rho = 166.080\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     248183296.0746852\n",
      "=====     9) : Lorenz rho = 166.090\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     14895899939.656153\n",
      "=====    10) : Lorenz rho = 166.100\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     41349580.71791326\n",
      "=====    11) : Lorenz rho = 166.110\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     67260433.07332222\n",
      "=====    12) : Lorenz rho = 166.120\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     650873068.3595302\n",
      "=====    13) : Lorenz rho = 166.130\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.8\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     36662020991.61536\n",
      "=====    14) : Lorenz rho = 166.140\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     24675116756.768566\n",
      "=====    15) : Lorenz rho = 166.150\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     12515703116.383223\n",
      "=====    16) : Lorenz rho = 166.160\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     3234035551.798558\n",
      "=====    17) : Lorenz rho = 166.170\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     7672952252.524497\n",
      "=====    18) : Lorenz rho = 166.180\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     13761044272.744278\n",
      "=====    19) : Lorenz rho = 166.190\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.8\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     2032257030.978441\n",
      "=====    20) : Lorenz rho = 166.200\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     897739163.2488961\n",
      "=====    21) : Lorenz rho = 166.210\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     5938758300.914709\n",
      "=====    22) : Lorenz rho = 166.220\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     30151528903.78262\n",
      "=====    23) : Lorenz rho = 166.230\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     11630774636.728994\n",
      "=====    24) : Lorenz rho = 166.240\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     30339385655.810005\n",
      "=====    25) : Lorenz rho = 166.250\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     3163613954.6819906\n",
      "=====    26) : Lorenz rho = 166.260\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.8\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     39196699289.86123\n",
      "=====    27) : Lorenz rho = 166.270\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     60492764428.9718\n",
      "=====    28) : Lorenz rho = 166.280\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     10446162754.604536\n",
      "=====    29) : Lorenz rho = 166.290\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.8\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     8451178775.089197\n",
      "=====    30) : Lorenz rho = 166.300\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     17617506348.559345\n",
      "=====    31) : Lorenz rho = 166.310\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     11317499231.357592\n",
      "=====    32) : Lorenz rho = 166.320\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     15684149236.131752\n",
      "=====    33) : Lorenz rho = 166.330\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     16206387683.704576\n",
      "=====    34) : Lorenz rho = 166.340\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.8\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     30224066053.676945\n",
      "=====    35) : Lorenz rho = 166.350\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     37419488532.70496\n",
      "=====    36) : Lorenz rho = 166.360\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     46159707296.35022\n",
      "=====    37) : Lorenz rho = 166.370\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     56230850057.24314\n",
      "=====    38) : Lorenz rho = 166.380\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     1615290169.0088089\n",
      "=====    39) : Lorenz rho = 166.390\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.8\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     16825855759.085238\n",
      "=====    40) : Lorenz rho = 166.400\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.8\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     13239464137.338165\n",
      "=====    41) : Lorenz rho = 166.410\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.8\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     13794870098.78547\n",
      "=====    42) : Lorenz rho = 166.420\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     17396844796.223007\n",
      "=====    43) : Lorenz rho = 166.430\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     1173856653.679079\n",
      "=====    44) : Lorenz rho = 166.440\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     28130948713.080402\n",
      "=====    45) : Lorenz rho = 166.450\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     55722067578.54693\n",
      "=====    46) : Lorenz rho = 166.460\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     14558267232.78905\n",
      "=====    47) : Lorenz rho = 166.470\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     8473928247.155405\n",
      "=====    48) : Lorenz rho = 166.480\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.8\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     27416647451.63236\n",
      "=====    49) : Lorenz rho = 166.490\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     16171232448.274588\n",
      "=====    50) : Lorenz rho = 166.500\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     11105166589.404049\n",
      "=====    51) : Lorenz rho = 166.510\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     10958579928.736166\n",
      "=====    52) : Lorenz rho = 166.520\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     8210268312.659136\n",
      "=====    53) : Lorenz rho = 166.530\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     32133567097.535683\n",
      "=====    54) : Lorenz rho = 166.540\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     7788970646.873617\n",
      "=====    55) : Lorenz rho = 166.550\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     9328493004.246317\n",
      "=====    56) : Lorenz rho = 166.560\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.8\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     73443420967.85844\n",
      "=====    57) : Lorenz rho = 166.570\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     159043029270.23212\n",
      "=====    58) : Lorenz rho = 166.580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.8\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     15636014864.295383\n",
      "=====    59) : Lorenz rho = 166.590\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     49998732798.19207\n",
      "=====    60) : Lorenz rho = 166.600\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.8\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     45448258891.38056\n",
      "=====    61) : Lorenz rho = 166.610\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     97920705727.20819\n",
      "=====    62) : Lorenz rho = 166.620\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.8\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     49785707609.19697\n",
      "=====    63) : Lorenz rho = 166.630\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     17513706806.53456\n",
      "=====    64) : Lorenz rho = 166.640\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     67806354092.29466\n",
      "=====    65) : Lorenz rho = 166.650\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.8\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     42446791845.886314\n",
      "=====    66) : Lorenz rho = 166.660\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     106475561669.73422\n",
      "=====    67) : Lorenz rho = 166.670\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.8\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     42100076460.539665\n",
      "=====    68) : Lorenz rho = 166.680\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     65267251851.61692\n",
      "=====    69) : Lorenz rho = 166.690\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     40388725299.55776\n",
      "=====    70) : Lorenz rho = 166.700\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     80388098791.0128\n",
      "=====    71) : Lorenz rho = 166.710\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     40651236091.65631\n",
      "=====    72) : Lorenz rho = 166.720\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     33670246596.41629\n",
      "=====    73) : Lorenz rho = 166.730\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     18761632306.64784\n",
      "=====    74) : Lorenz rho = 166.740\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     15227280134.868013\n",
      "=====    75) : Lorenz rho = 166.750\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     17220979082.984768\n",
      "=====    76) : Lorenz rho = 166.760\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     24609984287.216705\n",
      "=====    77) : Lorenz rho = 166.770\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     52209258689.64665\n",
      "=====    78) : Lorenz rho = 166.780\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     40548980320.345276\n",
      "=====    79) : Lorenz rho = 166.790\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.8\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     29376657012.297157\n",
      "=====    80) : Lorenz rho = 166.800\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     10018515443.820425\n",
      "=====    81) : Lorenz rho = 166.810\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     52595746853.81949\n",
      "=====    82) : Lorenz rho = 166.820\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     2362973968.6193857\n",
      "=====    83) : Lorenz rho = 166.830\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     3353797531.5472918\n",
      "=====    84) : Lorenz rho = 166.840\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     13905804774.73387\n",
      "=====    85) : Lorenz rho = 166.850\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.8\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     57119082853.15775\n",
      "=====    86) : Lorenz rho = 166.860\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     11640329628.08416\n",
      "=====    87) : Lorenz rho = 166.870\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     37068435290.41593\n",
      "=====    88) : Lorenz rho = 166.880\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     35502361687.08448\n",
      "=====    89) : Lorenz rho = 166.890\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     24415750172.373585\n",
      "=====    90) : Lorenz rho = 166.900\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     30912328853.06139\n",
      "=====    91) : Lorenz rho = 166.910\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     21433255463.57588\n",
      "=====    92) : Lorenz rho = 166.920\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     51765272200.78395\n",
      "=====    93) : Lorenz rho = 166.930\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     65583530113.467995\n",
      "=====    94) : Lorenz rho = 166.940\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.8\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     72716070814.00327\n",
      "=====    95) : Lorenz rho = 166.950\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     25518851080.129776\n",
      "=====    96) : Lorenz rho = 166.960\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     53923688470.864914\n",
      "=====    97) : Lorenz rho = 166.970\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.8\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     88200886121.26\n",
      "=====    98) : Lorenz rho = 166.980\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     57615926456.838936\n",
      "=====    99) : Lorenz rho = 166.990\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     17905399138.7539\n",
      "=====   100) : Lorenz rho = 167.000\n",
      "      ===== best hyperparameters\n",
      "            N        200\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     115321845366.2444\n"
     ]
    }
   ],
   "source": [
    "#######################################################################\n",
    "# O P T I M I Z A T I O N   L O O P \n",
    "#######################################################################\n",
    "rnd_seed = 167\n",
    "nome_file = 'df_RC200_seed' + str(rnd_seed) + '_HP_intermittency'\n",
    "s_from = 0\n",
    "s_to   = 101\n",
    "# store parameters and RC hyperparameters\n",
    "RC_HP   = np.zeros((s_to - s_from, 11)) #np.zeros((len_rho_list, 11))\n",
    "\n",
    "cnt = -1\n",
    "for s in range(s_from, s_to): #range(len_rho_list):\n",
    "    rho_Lorenz  = rho_list[s]\n",
    "    # read dataset\n",
    "    str_rho_Lorenz = (f'{rho_Lorenz:07.3f}').replace(\".\", \"_\")\n",
    "    print(\"===== \",f'{s:>4d}) : Lorenz rho = {rho_Lorenz:>7.3f}')\n",
    "    X = np.loadtxt('dataset2/Lorenz_Dataset_'+str(str_rho_Lorenz)+'.csv',delimiter=\";\")\n",
    "    n_timesteps = len(X)\n",
    "    #print(n_timesteps)\n",
    "    \n",
    "    # splitting in training and test dataset\n",
    "    data_length          = len(X) \n",
    "    training_percentage  = .8\n",
    "    training_data_length = round(training_percentage * data_length) \n",
    "    #print(\"data_length          =\",data_length)\n",
    "    #print(\"training_data_length =\",training_data_length)\n",
    "    training_data = np.array(X[:training_data_length])\n",
    "    test_data     = np.array(X[training_data_length:])\n",
    "    \n",
    "    # Setting random seed for repeteability\n",
    "    np.random.seed(rnd_seed)\n",
    "    random.seed(rnd_seed)\n",
    "    \n",
    "    # Set random forecast spinup list\n",
    "    M_forecasts = 100\n",
    "    S_forecast_steps = 500\n",
    "    max_spinup  = len(test_data) - S_forecast_steps\n",
    "    spinup_list = np.zeros(M_forecasts, dtype=np.uint32)\n",
    "    for i in range(M_forecasts):\n",
    "        spinup_list[i] = round(max_spinup * np.random.rand())    \n",
    "    \n",
    "    for i in range(len_vec):\n",
    "        # Setting random seed for repeteability\n",
    "        np.random.seed(rnd_seed)\n",
    "        random.seed(rnd_seed)\n",
    "        # set hyperparameter from cartesian product\n",
    "        D      = 3\n",
    "        N      = cart_prod[i][0]\n",
    "        rhoSR  = cart_prod[i][1]\n",
    "        rhoA   = cart_prod[i][2]\n",
    "        alpha  = cart_prod[i][3]  \n",
    "        sigma  = cart_prod[i][4]\n",
    "        sigmab = cart_prod[i][5] \n",
    "        beta   = cart_prod[i][6]\n",
    "        #\n",
    "        model   = ReservoirComputer(D, N, rhoSR, rhoA, alpha, sigma, sigmab, beta)\n",
    "        # training\n",
    "        washout = 100 # transitory skipped timesteps\n",
    "        model.train(training_data, washout)\n",
    "        # compute loss macro value\n",
    "        vec_loss_macro[i] = loss_macro(model,test_data,spinup_list,S_forecast_steps)\n",
    "        #print(\"i =\",i, vec_loss_macro[i])\n",
    "        \n",
    "        del model\n",
    "        \n",
    "    idx_vec_loss_macro = np.argmin(vec_loss_macro, axis=-1)\n",
    "    print(\"      ===== best hyperparameters\")\n",
    "    print(\"            N       \",cart_prod[idx_vec_loss_macro][0])\n",
    "    print(\"            rhoSR   \",cart_prod[idx_vec_loss_macro][1])\n",
    "    print(\"            rhoA    \",cart_prod[idx_vec_loss_macro][2])\n",
    "    print(\"            alpha   \",cart_prod[idx_vec_loss_macro][3])\n",
    "    print(\"            sigma   \",cart_prod[idx_vec_loss_macro][4])\n",
    "    print(\"            sigmab  \",cart_prod[idx_vec_loss_macro][5])\n",
    "    print(\"            beta    \",cart_prod[idx_vec_loss_macro][6])\n",
    "    print(\"            loss    \",vec_loss_macro[idx_vec_loss_macro])\n",
    "\n",
    "    # store best hyperparameters\n",
    "    cnt += 1\n",
    "    RC_HP[cnt][0]  = rho_Lorenz\n",
    "    RC_HP[cnt][1]  = D\n",
    "    RC_HP[cnt][2]  = washout\n",
    "    RC_HP[cnt][3]  = cart_prod[idx_vec_loss_macro][0]\n",
    "    RC_HP[cnt][4]  = cart_prod[idx_vec_loss_macro][1]\n",
    "    RC_HP[cnt][5]  = cart_prod[idx_vec_loss_macro][2]\n",
    "    RC_HP[cnt][6]  = cart_prod[idx_vec_loss_macro][3]\n",
    "    RC_HP[cnt][7]  = cart_prod[idx_vec_loss_macro][4]\n",
    "    RC_HP[cnt][8]  = cart_prod[idx_vec_loss_macro][5]\n",
    "    RC_HP[cnt][9]  = cart_prod[idx_vec_loss_macro][6]\n",
    "    RC_HP[cnt][10] = vec_loss_macro[idx_vec_loss_macro]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26b060c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy array to pandas DataFrame\n",
    "df_RC_HP = pd.DataFrame(RC_HP)\n",
    "\n",
    "# Name columns\n",
    "df_RC_HP.columns =['rho_Lorenz',\n",
    "                   'D',\n",
    "                   'washout',\n",
    "                   'N',        \n",
    "                   'rhoSR',   \n",
    "                   'rhoA',    \n",
    "                   'alpha',    \n",
    "                   'sigma',    \n",
    "                   'sigmab',   \n",
    "                   'beta',\n",
    "                   'loss']\n",
    "\n",
    "# Save DataFrame to .csv\n",
    "df_RC_HP.to_csv('climate/'+ nome_file + '.csv', index=False, header=True, decimal='.', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60fe6ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Elapsed time 2.83e+04 s\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------------------------------------------#---------------------------------------------------------------------#\n",
    "# Elapsed time\n",
    "#---------------------------------------------------------------------#\n",
    "print(f'\\nElapsed time {time.time() - start_time:6.2e} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1fadde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
