{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "556b81cf",
   "metadata": {},
   "source": [
    "# Standard Reservoir Computer (paper version)\n",
    "\n",
    "### hyperparameters optimization\n",
    "\n",
    "### ( datasets by $\\rho \\in A =$ list of values  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae665baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import math     as math\n",
    "import numpy    as np\n",
    "import networkx as nx\n",
    "import random   as random\n",
    "import pandas   as pd\n",
    "import time     as time\n",
    "\n",
    "#######################################################################\n",
    "# E N V I R O N M E N T   S E T   U P\n",
    "#######################################################################\n",
    "#---------------------------------------------------------------------#\n",
    "# To compute elapsed time\n",
    "#---------------------------------------------------------------------#\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5bb9f7",
   "metadata": {},
   "source": [
    "## variables description\n",
    "    D       :(int  ) Input data dimension      \n",
    "    N       :(int  ) Reservoir dimension (degrees of freedome of the reservoir)       \n",
    "    rhoSR   :(float) Spectral Radius of A      \n",
    "    rhoA    :(float) Density of A              \n",
    "    alpha   :(float) Leak (or leakage) rate in (0,1]                 \n",
    "    sigma   :(float) Strength of input signal            \n",
    "    sigmab  :(float) Strength of input bias               \n",
    "    beta    :(float) Tichonov-Miller regularization parameter\n",
    "    \n",
    "    washout :(int)   During training, skipped transitory timesteps in W_out calculation\n",
    "    spinup  :(int)   Time (n. of timesteps) it takes for a trained RC to converge from its initial condition\n",
    "                     onto the synchronization manifold to which it is driven by the input data\n",
    "    normtime:(int)   Range to skip some QR factorisations and speed up the calculations  \n",
    "            \n",
    "    r       :(float) Reservoir state\n",
    "    W_in    :(float) Input matrix              \n",
    "    A       :(float) Reservoir ajacency matrix \n",
    "    b       :(float) bias vector               \n",
    "    W_out   :(float) Output matrix  \n",
    "    endtr_r :(float) Last reservoir state in training\n",
    "    \n",
    "    R       :(float) Matrix containing r(t) for all t in training dataset\n",
    "    U       :(float) Matrix containing u(t) for all t in training dataset\n",
    "    u       :(float) Time series at time t\n",
    "    v       :(float) Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "129942da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReservoirComputer class declaration\n",
    "class ReservoirComputer:\n",
    "    def __init__(self, D, N, rhoSR, rhoA, alpha, sigma, sigmab, beta):\n",
    "        self.r      = np.zeros(N)\n",
    "        self.W_in   = get_random_matrix(N, D, xa=-sigma, xb=sigma, nonzero=False)\n",
    "        self.A      = generate_adjacency_matrix(N, rhoSR, rhoA)\n",
    "        self.b      = sigmab * np.ones(N)\n",
    "        self.W_out  = np.zeros((D,N))\n",
    "        self.endtr_r= np.zeros(N)\n",
    "\n",
    "    def rc_update_rule(self, y):\n",
    "        # driven     mode: y = u(t)         ;r(t+1) = F^d_r (r(t),y)\n",
    "        # autonomous mode: y = W_out . r(t) ;r(t+1) = F^a_r (r(t),y)\n",
    "        g      = np.dot(self.A, self.r) + np.dot(self.W_in, y) + self.b\n",
    "        self.r = alpha * np.tanh(g) + (1 - alpha) * self.r\n",
    "\n",
    "    def update_v(self):\n",
    "        return np.dot(self.W_out, self.r)\n",
    "\n",
    "    def train(self, U, washout):\n",
    "        steps = U.shape[0]\n",
    "        R     = np.zeros((N, steps))\n",
    "        for i in range(steps):\n",
    "            R[:, i] = self.r\n",
    "            u       = U[i]\n",
    "            self.rc_update_rule(u)\n",
    "        self.W_out = linear_regression(R[:,washout:], U[washout:], beta)\n",
    "        self.endtr_r = self.r # save last training r state\n",
    "\n",
    "    def spinup(self, U, steps):\n",
    "        self.r = self.endtr_r # reset reservoir state\n",
    "        if steps > 0:\n",
    "            for i in range(steps):\n",
    "                u = U[i]\n",
    "                self.rc_update_rule(u) # driven mode\n",
    "    \n",
    "    def predict(self, steps):\n",
    "        prediction = np.zeros((steps, D))\n",
    "        for i in range(steps):\n",
    "            v             = self.update_v()\n",
    "            prediction[i] = v\n",
    "            self.rc_update_rule(v)\n",
    "        return prediction\n",
    "\n",
    "    def rc_lyapunov_exponents(self, steps, dt, normtime):\n",
    "        save_r = self.r # save r state\n",
    "        self.r = self.endtr_r # reset r state\n",
    "        lyap   = np.zeros((N,steps))\n",
    "        M      = np.eye(N)\n",
    "        W      = self.A + np.dot(self.W_in,self.W_out)\n",
    "        j      = -1\n",
    "        for i in range(steps):\n",
    "            v     = self.update_v()\n",
    "            self.rc_update_rule(v) # update r\n",
    "            #\n",
    "            g     = np.dot(W, self.r) + self.b\n",
    "            DF    = alpha * np.dot(np.diag(1 - np.tanh(g)**2),W) \\\n",
    "                    + (1 - alpha) * np.eye(N)\n",
    "            Mn    = np.dot(DF,M)\n",
    "            if (i % normtime == 0):\n",
    "                Q,Rii = np.linalg.qr(Mn)\n",
    "                j     += 1\n",
    "                lyap[:,j] = np.log(abs(np.diag(Rii)))\n",
    "                M = Q\n",
    "        L = np.sum(lyap,1) / ((j+1)*dt)    \n",
    "        self.r = save_r # retrieve saved r_state\n",
    "        return L\n",
    "\n",
    "    def rc_conditional_lyapunov_exponents(self, U, dt, normtime):\n",
    "        save_r = self.r # save r state\n",
    "        self.r = np.zeros(N) # reset r state\n",
    "        steps  = U.shape[0]\n",
    "        lyap   = np.zeros((N,steps))\n",
    "        M      = np.eye(N)\n",
    "        j      = -1\n",
    "        for i in range(steps):\n",
    "            u       = U[i]\n",
    "            self.rc_update_rule(u) # update r\n",
    "            #\n",
    "            g  = np.dot(self.A, self.r) + np.dot(self.W_in, u) + self.b\n",
    "            DF = alpha * np.dot(np.diag(1 - np.tanh(g)**2),self.A) \\\n",
    "                 + (1 - alpha) * np.eye(N)\n",
    "            Mn = np.dot(DF,M)\n",
    "            if (i % normtime == 0):\n",
    "                Q,Rii = np.linalg.qr(Mn)\n",
    "                j     += 1\n",
    "                lyap[:,j] = np.log(abs(np.diag(Rii)))\n",
    "                M = Q\n",
    "        CL = np.sum(lyap,1) / ((j+1)*dt)\n",
    "        self.r = save_r # retrieve saved r_state\n",
    "        return CL\n",
    "    \n",
    "# Helper functions\n",
    "def generate_adjacency_matrix(N, rhoSR, rhoA):\n",
    "    # Erdos-Reyni network\n",
    "    graph = nx.gnp_random_graph(N, rhoA)\n",
    "    adj   = nx.to_numpy_array(graph)\n",
    "    # Ensure random_array is of the same shape as the graph adjacency matrix\n",
    "    random_array = get_random_matrix(N, N, xa =-0.5, xb=0.5, nonzero=True)\n",
    "    # Multiply graph adjacency matrix with random values\n",
    "    rescaled     = adj * random_array\n",
    "    return scale_matrix(rescaled, rhoSR)\n",
    "\n",
    "def get_random_matrix(nrow, ncol, xa, xb, nonzero):\n",
    "    B = np.zeros((nrow,ncol))\n",
    "    for i in range(nrow):\n",
    "        for j in range(ncol):\n",
    "            if nonzero:\n",
    "                while B[i,j] == 0:\n",
    "                    B[i,j] = xa + (xb - xa) * np.random.rand()\n",
    "            else:\n",
    "                B[i,j] = xa + (xb - xa) * np.random.rand()\n",
    "    return B\n",
    "\n",
    "def scale_matrix(A, rhoSR):\n",
    "    eigenvalues = np.linalg.eigvals(A)    # compute eigenvlaues\n",
    "    sr = np.max(np.absolute(eigenvalues)) # compute spectral radius of A\n",
    "    if sr > 0:\n",
    "        A = A * rhoSR / sr                # rescaling matrix if A non zero\n",
    "    return A\n",
    "\n",
    "def linear_regression(R, U, beta=.0001): \n",
    "    Rt = np.transpose(R)\n",
    "    inverse_part = np.linalg.inv(np.dot(R, Rt) + beta * np.identity(R.shape[0]))\n",
    "    return np.dot(np.dot(U.T, Rt), inverse_part)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f1b2277",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# O P T I M I Z A T I O N   L O S S   F U N C T I O N \n",
    "#######################################################################\n",
    "# spinup_list = list of spinups values for forecasting \n",
    "def loss_macro(RC,test_data,spinup_list,forecast_steps):\n",
    "    loss_value = 0.\n",
    "    for i in range(len(spinup_list)):\n",
    "        spinup_steps = spinup_list[i]\n",
    "        RC.spinup(test_data, spinup_steps)\n",
    "        #\n",
    "        pred_data = RC.predict(forecast_steps)\n",
    "        #\n",
    "        for j in range(forecast_steps):\n",
    "            loss_value += np.linalg.norm(test_data[spinup_steps+j] - pred_data[j])**2 \\\n",
    "                        * np.exp(- (j+1) / (forecast_steps) )\n",
    "    return loss_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42a8094c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HP list length = 36\n"
     ]
    }
   ],
   "source": [
    "#######################################################################\n",
    "# H Y P E R P A R A M E T E R S   L I S T \n",
    "#######################################################################\n",
    "N_list      = [400]\n",
    "rhoSR_list  = [0.2, 0.5, 0.8]\n",
    "rhoA_list   = [0.02]\n",
    "alpha_list  = [0.4, 0.6, 0.8]\n",
    "sigma_list  = [0.084]\n",
    "sigmab_list = [1.1, 1.3, 1.5, 1.7]\n",
    "beta_list   = [8.5e-8]\n",
    "\n",
    "cart_prod = [(a,b,c,d,e,f,g)\n",
    "             for a in N_list       \n",
    "             for b in rhoSR_list   \n",
    "             for c in rhoA_list    \n",
    "             for d in alpha_list   \n",
    "             for e in sigma_list   \n",
    "             for f in sigmab_list  \n",
    "             for g in beta_list   ]\n",
    "\n",
    "len_vec = len(cart_prod)\n",
    "vec_loss_macro = np.zeros(len_vec)\n",
    "\n",
    "print(\"HP list length =\",len_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae59a5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342\n"
     ]
    }
   ],
   "source": [
    "#######################################################################\n",
    "# L I S T   O F   R H O   V A L U E S\n",
    "#######################################################################\n",
    "rho_list  = np.arange(0, 331, 1)\n",
    "\n",
    "rho_list  = np.append(rho_list, np.array([13.926667, 23.9, 24.058, 470./19., 30.485]))\n",
    "rho_list  = np.append(rho_list, np.array([99.524, 100.795]))\n",
    "rho_list  = np.append(rho_list, np.array([148.4, 166.07, 214.364, 233.5]))\n",
    "\n",
    "rho_list  = np.sort(rho_list)\n",
    "\n",
    "len_rho_list = len(rho_list)\n",
    "\n",
    "print(len_rho_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b113a7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====   120) : Lorenz rho = 113.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     40380289.11529635\n",
      "=====   121) : Lorenz rho = 114.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.8\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     400150.04425778025\n",
      "=====   122) : Lorenz rho = 115.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     43601411.7637809\n",
      "=====   123) : Lorenz rho = 116.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     22039386.11151285\n",
      "=====   124) : Lorenz rho = 117.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     28378489.043280132\n",
      "=====   125) : Lorenz rho = 118.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     56782906.77381552\n",
      "=====   126) : Lorenz rho = 119.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     48023878.8646384\n",
      "=====   127) : Lorenz rho = 120.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     46339204.46654739\n",
      "=====   128) : Lorenz rho = 121.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     30374759.014534\n",
      "=====   129) : Lorenz rho = 122.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     22576287.126373187\n",
      "=====   130) : Lorenz rho = 123.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     24513711.17025616\n",
      "=====   131) : Lorenz rho = 124.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     46994003.15055375\n",
      "=====   132) : Lorenz rho = 125.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     44920023.38246137\n",
      "=====   133) : Lorenz rho = 126.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     35306090.89615036\n",
      "=====   134) : Lorenz rho = 127.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     40733161.5693659\n",
      "=====   135) : Lorenz rho = 128.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     25694926.922021046\n",
      "=====   136) : Lorenz rho = 129.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     53214549.99565421\n",
      "=====   137) : Lorenz rho = 130.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     51895203.97181497\n",
      "=====   138) : Lorenz rho = 131.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     38167456.11683588\n",
      "=====   139) : Lorenz rho = 132.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     59540.96346011995\n",
      "=====   140) : Lorenz rho = 133.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     176107.23264945153\n",
      "=====   141) : Lorenz rho = 134.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     59163753.458034396\n",
      "=====   142) : Lorenz rho = 135.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.8\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     65969622.025896266\n",
      "=====   143) : Lorenz rho = 136.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     162729376.85421154\n",
      "=====   144) : Lorenz rho = 137.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     109486866.77588814\n",
      "=====   145) : Lorenz rho = 138.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     196395075.69924885\n",
      "=====   146) : Lorenz rho = 139.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     240752191.5900878\n",
      "=====   147) : Lorenz rho = 140.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     65822015.098329395\n",
      "=====   148) : Lorenz rho = 141.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     73369635.00123563\n",
      "=====   149) : Lorenz rho = 142.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     48647600.712209284\n",
      "=====   150) : Lorenz rho = 143.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     3587544.811692887\n",
      "=====   151) : Lorenz rho = 144.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     1066700.2167386943\n",
      "=====   152) : Lorenz rho = 145.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     1163228.082097927\n",
      "=====   153) : Lorenz rho = 146.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     356697.9906086468\n",
      "=====   154) : Lorenz rho = 147.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.8\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     130813.41502480293\n",
      "=====   155) : Lorenz rho = 148.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     32089.3093100042\n",
      "=====   156) : Lorenz rho = 148.400\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     127851.33675659438\n",
      "=====   157) : Lorenz rho = 149.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     28080.100399375253\n",
      "=====   158) : Lorenz rho = 150.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     5522.9609784000095\n",
      "=====   159) : Lorenz rho = 151.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     8761.027363543544\n",
      "=====   160) : Lorenz rho = 152.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.8\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     4197.249298811263\n",
      "=====   161) : Lorenz rho = 153.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     78476.32467277693\n",
      "=====   162) : Lorenz rho = 154.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     83022.51115822769\n",
      "=====   163) : Lorenz rho = 155.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     25981.30633611294\n",
      "=====   164) : Lorenz rho = 156.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     1321609.474830646\n",
      "=====   165) : Lorenz rho = 157.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     429339.7162071336\n",
      "=====   166) : Lorenz rho = 158.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     248290.5992038448\n",
      "=====   167) : Lorenz rho = 159.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     27310.16773527222\n",
      "=====   168) : Lorenz rho = 160.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     142143.76405044826\n",
      "=====   169) : Lorenz rho = 161.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     48929.114808859864\n",
      "=====   170) : Lorenz rho = 162.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     18047.9118767028\n",
      "=====   171) : Lorenz rho = 163.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     250003.3541617085\n",
      "=====   172) : Lorenz rho = 164.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     5096.227171114391\n",
      "=====   173) : Lorenz rho = 165.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     369662.9773266617\n",
      "=====   174) : Lorenz rho = 166.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     189620.14311385842\n",
      "=====   175) : Lorenz rho = 166.070\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.8\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     101752396.55083239\n",
      "=====   176) : Lorenz rho = 167.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     729360192.3184005\n",
      "=====   177) : Lorenz rho = 168.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     703842680.2118932\n",
      "=====   178) : Lorenz rho = 169.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     2259978644.6064157\n",
      "=====   179) : Lorenz rho = 170.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     755448037.6670564\n"
     ]
    }
   ],
   "source": [
    "#######################################################################\n",
    "# O P T I M I Z A T I O N   L O O P \n",
    "#######################################################################\n",
    "rnd_seed = 167\n",
    "nome_file = 'df_RC400_seed' + str(rnd_seed) + '_HP_parte03'\n",
    "s_from = 120\n",
    "s_to   = 180\n",
    "# store parameters and RC hyperparameters\n",
    "RC_HP   = np.zeros((s_to - s_from, 11)) #np.zeros((len_rho_list, 11))\n",
    "\n",
    "cnt = -1\n",
    "for s in range(s_from, s_to): #range(len_rho_list):\n",
    "    rho_Lorenz  = rho_list[s]\n",
    "    # read dataset\n",
    "    str_rho_Lorenz = (f'{rho_Lorenz:07.3f}').replace(\".\", \"_\")\n",
    "    print(\"===== \",f'{s:>4d}) : Lorenz rho = {rho_Lorenz:>7.3f}')\n",
    "    X = np.loadtxt('dataset/Lorenz_Dataset_'+str(str_rho_Lorenz)+'.csv',delimiter=\";\")\n",
    "    n_timesteps = len(X)\n",
    "    #print(n_timesteps)\n",
    "    \n",
    "    # splitting in training and test dataset\n",
    "    data_length          = len(X) \n",
    "    training_percentage  = .8\n",
    "    training_data_length = round(training_percentage * data_length) \n",
    "    #print(\"data_length          =\",data_length)\n",
    "    #print(\"training_data_length =\",training_data_length)\n",
    "    training_data = np.array(X[:training_data_length])\n",
    "    test_data     = np.array(X[training_data_length:])\n",
    "    \n",
    "    # Setting random seed for repeteability\n",
    "    \n",
    "    np.random.seed(rnd_seed)\n",
    "    random.seed(rnd_seed)\n",
    "    \n",
    "    # Set random forecast spinup list\n",
    "    M_forecasts = 100\n",
    "    S_forecast_steps = 500\n",
    "    max_spinup  = len(test_data) - S_forecast_steps\n",
    "    spinup_list = np.zeros(M_forecasts, dtype=np.uint32)\n",
    "    for i in range(M_forecasts):\n",
    "        spinup_list[i] = round(max_spinup * np.random.rand())    \n",
    "    \n",
    "    for i in range(len_vec):\n",
    "        # Setting random seed for repeteability\n",
    "        np.random.seed(rnd_seed)\n",
    "        random.seed(rnd_seed)\n",
    "        # set hyperparameter from cartesian product\n",
    "        D      = 3\n",
    "        N      = cart_prod[i][0]\n",
    "        rhoSR  = cart_prod[i][1]\n",
    "        rhoA   = cart_prod[i][2]\n",
    "        alpha  = cart_prod[i][3]  \n",
    "        sigma  = cart_prod[i][4]\n",
    "        sigmab = cart_prod[i][5] \n",
    "        beta   = cart_prod[i][6]\n",
    "        #\n",
    "        model   = ReservoirComputer(D, N, rhoSR, rhoA, alpha, sigma, sigmab, beta)\n",
    "        # training\n",
    "        washout = 100 # transitory skipped timesteps\n",
    "        model.train(training_data, washout)\n",
    "        # compute loss macro value\n",
    "        vec_loss_macro[i] = loss_macro(model,test_data,spinup_list,S_forecast_steps)\n",
    "        #print(\"i =\",i, vec_loss_macro[i])\n",
    "        \n",
    "        del model\n",
    "        \n",
    "    idx_vec_loss_macro = np.argmin(vec_loss_macro, axis=-1)\n",
    "    print(\"      ===== best hyperparameters\")\n",
    "    print(\"            N       \",cart_prod[idx_vec_loss_macro][0])\n",
    "    print(\"            rhoSR   \",cart_prod[idx_vec_loss_macro][1])\n",
    "    print(\"            rhoA    \",cart_prod[idx_vec_loss_macro][2])\n",
    "    print(\"            alpha   \",cart_prod[idx_vec_loss_macro][3])\n",
    "    print(\"            sigma   \",cart_prod[idx_vec_loss_macro][4])\n",
    "    print(\"            sigmab  \",cart_prod[idx_vec_loss_macro][5])\n",
    "    print(\"            beta    \",cart_prod[idx_vec_loss_macro][6])\n",
    "    print(\"            loss    \",vec_loss_macro[idx_vec_loss_macro])\n",
    "\n",
    "    # store best hyperparameters\n",
    "    cnt += 1\n",
    "    RC_HP[cnt][0]  = rho_Lorenz\n",
    "    RC_HP[cnt][1]  = D\n",
    "    RC_HP[cnt][2]  = washout\n",
    "    RC_HP[cnt][3]  = cart_prod[idx_vec_loss_macro][0]\n",
    "    RC_HP[cnt][4]  = cart_prod[idx_vec_loss_macro][1]\n",
    "    RC_HP[cnt][5]  = cart_prod[idx_vec_loss_macro][2]\n",
    "    RC_HP[cnt][6]  = cart_prod[idx_vec_loss_macro][3]\n",
    "    RC_HP[cnt][7]  = cart_prod[idx_vec_loss_macro][4]\n",
    "    RC_HP[cnt][8]  = cart_prod[idx_vec_loss_macro][5]\n",
    "    RC_HP[cnt][9]  = cart_prod[idx_vec_loss_macro][6]\n",
    "    RC_HP[cnt][10] = vec_loss_macro[idx_vec_loss_macro]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26b060c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy array to pandas DataFrame\n",
    "df_RC_HP = pd.DataFrame(RC_HP)\n",
    "\n",
    "# Name columns\n",
    "df_RC_HP.columns =['rho_Lorenz',\n",
    "                   'D',\n",
    "                   'washout',\n",
    "                   'N',        \n",
    "                   'rhoSR',   \n",
    "                   'rhoA',    \n",
    "                   'alpha',    \n",
    "                   'sigma',    \n",
    "                   'sigmab',   \n",
    "                   'beta',\n",
    "                   'loss']\n",
    "\n",
    "# Save DataFrame to .csv\n",
    "df_RC_HP.to_csv('climate/'+ nome_file + '.csv', index=False, header=True, decimal='.', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60fe6ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Elapsed time 2.77e+04 s\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------------------------------------------#---------------------------------------------------------------------#\n",
    "# Elapsed time\n",
    "#---------------------------------------------------------------------#\n",
    "print(f'\\nElapsed time {time.time() - start_time:6.2e} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1fadde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
