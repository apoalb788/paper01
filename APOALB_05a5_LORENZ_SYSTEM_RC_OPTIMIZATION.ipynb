{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "556b81cf",
   "metadata": {},
   "source": [
    "# Standard Reservoir Computer (paper version)\n",
    "\n",
    "### hyperparameters optimization\n",
    "\n",
    "### ( datasets by $\\rho \\in A =$ list of values  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae665baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import math     as math\n",
    "import numpy    as np\n",
    "import networkx as nx\n",
    "import random   as random\n",
    "import pandas   as pd\n",
    "import time     as time\n",
    "\n",
    "#######################################################################\n",
    "# E N V I R O N M E N T   S E T   U P\n",
    "#######################################################################\n",
    "#---------------------------------------------------------------------#\n",
    "# To compute elapsed time\n",
    "#---------------------------------------------------------------------#\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5bb9f7",
   "metadata": {},
   "source": [
    "## variables description\n",
    "    D       :(int  ) Input data dimension      \n",
    "    N       :(int  ) Reservoir dimension (degrees of freedome of the reservoir)       \n",
    "    rhoSR   :(float) Spectral Radius of A      \n",
    "    rhoA    :(float) Density of A              \n",
    "    alpha   :(float) Leak (or leakage) rate in (0,1]                 \n",
    "    sigma   :(float) Strength of input signal            \n",
    "    sigmab  :(float) Strength of input bias               \n",
    "    beta    :(float) Tichonov-Miller regularization parameter\n",
    "    \n",
    "    washout :(int)   During training, skipped transitory timesteps in W_out calculation\n",
    "    spinup  :(int)   Time (n. of timesteps) it takes for a trained RC to converge from its initial condition\n",
    "                     onto the synchronization manifold to which it is driven by the input data\n",
    "    normtime:(int)   Range to skip some QR factorisations and speed up the calculations  \n",
    "            \n",
    "    r       :(float) Reservoir state\n",
    "    W_in    :(float) Input matrix              \n",
    "    A       :(float) Reservoir ajacency matrix \n",
    "    b       :(float) bias vector               \n",
    "    W_out   :(float) Output matrix  \n",
    "    endtr_r :(float) Last reservoir state in training\n",
    "    \n",
    "    R       :(float) Matrix containing r(t) for all t in training dataset\n",
    "    U       :(float) Matrix containing u(t) for all t in training dataset\n",
    "    u       :(float) Time series at time t\n",
    "    v       :(float) Prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "129942da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReservoirComputer class declaration\n",
    "class ReservoirComputer:\n",
    "    def __init__(self, D, N, rhoSR, rhoA, alpha, sigma, sigmab, beta):\n",
    "        self.r      = np.zeros(N)\n",
    "        self.W_in   = get_random_matrix(N, D, xa=-sigma, xb=sigma, nonzero=False)\n",
    "        self.A      = generate_adjacency_matrix(N, rhoSR, rhoA)\n",
    "        self.b      = sigmab * np.ones(N)\n",
    "        self.W_out  = np.zeros((D,N))\n",
    "        self.endtr_r= np.zeros(N)\n",
    "\n",
    "    def rc_update_rule(self, y):\n",
    "        # driven     mode: y = u(t)         ;r(t+1) = F^d_r (r(t),y)\n",
    "        # autonomous mode: y = W_out . r(t) ;r(t+1) = F^a_r (r(t),y)\n",
    "        g      = np.dot(self.A, self.r) + np.dot(self.W_in, y) + self.b\n",
    "        self.r = alpha * np.tanh(g) + (1 - alpha) * self.r\n",
    "\n",
    "    def update_v(self):\n",
    "        return np.dot(self.W_out, self.r)\n",
    "\n",
    "    def train(self, U, washout):\n",
    "        steps = U.shape[0]\n",
    "        R     = np.zeros((N, steps))\n",
    "        for i in range(steps):\n",
    "            R[:, i] = self.r\n",
    "            u       = U[i]\n",
    "            self.rc_update_rule(u)\n",
    "        self.W_out = linear_regression(R[:,washout:], U[washout:], beta)\n",
    "        self.endtr_r = self.r # save last training r state\n",
    "\n",
    "    def spinup(self, U, steps):\n",
    "        self.r = self.endtr_r # reset reservoir state\n",
    "        if steps > 0:\n",
    "            for i in range(steps):\n",
    "                u = U[i]\n",
    "                self.rc_update_rule(u) # driven mode\n",
    "    \n",
    "    def predict(self, steps):\n",
    "        prediction = np.zeros((steps, D))\n",
    "        for i in range(steps):\n",
    "            v             = self.update_v()\n",
    "            prediction[i] = v\n",
    "            self.rc_update_rule(v)\n",
    "        return prediction\n",
    "\n",
    "    def rc_lyapunov_exponents(self, steps, dt, normtime):\n",
    "        save_r = self.r # save r state\n",
    "        self.r = self.endtr_r # reset r state\n",
    "        lyap   = np.zeros((N,steps))\n",
    "        M      = np.eye(N)\n",
    "        W      = self.A + np.dot(self.W_in,self.W_out)\n",
    "        j      = -1\n",
    "        for i in range(steps):\n",
    "            v     = self.update_v()\n",
    "            self.rc_update_rule(v) # update r\n",
    "            #\n",
    "            g     = np.dot(W, self.r) + self.b\n",
    "            DF    = alpha * np.dot(np.diag(1 - np.tanh(g)**2),W) \\\n",
    "                    + (1 - alpha) * np.eye(N)\n",
    "            Mn    = np.dot(DF,M)\n",
    "            if (i % normtime == 0):\n",
    "                Q,Rii = np.linalg.qr(Mn)\n",
    "                j     += 1\n",
    "                lyap[:,j] = np.log(abs(np.diag(Rii)))\n",
    "                M = Q\n",
    "        L = np.sum(lyap,1) / ((j+1)*dt)    \n",
    "        self.r = save_r # retrieve saved r_state\n",
    "        return L\n",
    "\n",
    "    def rc_conditional_lyapunov_exponents(self, U, dt, normtime):\n",
    "        save_r = self.r # save r state\n",
    "        self.r = np.zeros(N) # reset r state\n",
    "        steps  = U.shape[0]\n",
    "        lyap   = np.zeros((N,steps))\n",
    "        M      = np.eye(N)\n",
    "        j      = -1\n",
    "        for i in range(steps):\n",
    "            u       = U[i]\n",
    "            self.rc_update_rule(u) # update r\n",
    "            #\n",
    "            g  = np.dot(self.A, self.r) + np.dot(self.W_in, u) + self.b\n",
    "            DF = alpha * np.dot(np.diag(1 - np.tanh(g)**2),self.A) \\\n",
    "                 + (1 - alpha) * np.eye(N)\n",
    "            Mn = np.dot(DF,M)\n",
    "            if (i % normtime == 0):\n",
    "                Q,Rii = np.linalg.qr(Mn)\n",
    "                j     += 1\n",
    "                lyap[:,j] = np.log(abs(np.diag(Rii)))\n",
    "                M = Q\n",
    "        CL = np.sum(lyap,1) / ((j+1)*dt)\n",
    "        self.r = save_r # retrieve saved r_state\n",
    "        return CL\n",
    "    \n",
    "# Helper functions\n",
    "def generate_adjacency_matrix(N, rhoSR, rhoA):\n",
    "    # Erdos-Reyni network\n",
    "    graph = nx.gnp_random_graph(N, rhoA)\n",
    "    adj   = nx.to_numpy_array(graph)\n",
    "    # Ensure random_array is of the same shape as the graph adjacency matrix\n",
    "    random_array = get_random_matrix(N, N, xa =-0.5, xb=0.5, nonzero=True)\n",
    "    # Multiply graph adjacency matrix with random values\n",
    "    rescaled     = adj * random_array\n",
    "    return scale_matrix(rescaled, rhoSR)\n",
    "\n",
    "def get_random_matrix(nrow, ncol, xa, xb, nonzero):\n",
    "    B = np.zeros((nrow,ncol))\n",
    "    for i in range(nrow):\n",
    "        for j in range(ncol):\n",
    "            if nonzero:\n",
    "                while B[i,j] == 0:\n",
    "                    B[i,j] = xa + (xb - xa) * np.random.rand()\n",
    "            else:\n",
    "                B[i,j] = xa + (xb - xa) * np.random.rand()\n",
    "    return B\n",
    "\n",
    "def scale_matrix(A, rhoSR):\n",
    "    eigenvalues = np.linalg.eigvals(A)    # compute eigenvlaues\n",
    "    sr = np.max(np.absolute(eigenvalues)) # compute spectral radius of A\n",
    "    if sr > 0:\n",
    "        A = A * rhoSR / sr                # rescaling matrix if A non zero\n",
    "    return A\n",
    "\n",
    "def linear_regression(R, U, beta=.0001): \n",
    "    Rt = np.transpose(R)\n",
    "    inverse_part = np.linalg.inv(np.dot(R, Rt) + beta * np.identity(R.shape[0]))\n",
    "    return np.dot(np.dot(U.T, Rt), inverse_part)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f1b2277",
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "# O P T I M I Z A T I O N   L O S S   F U N C T I O N \n",
    "#######################################################################\n",
    "# spinup_list = list of spinups values for forecasting \n",
    "def loss_macro(RC,test_data,spinup_list,forecast_steps):\n",
    "    loss_value = 0.\n",
    "    for i in range(len(spinup_list)):\n",
    "        spinup_steps = spinup_list[i]\n",
    "        RC.spinup(test_data, spinup_steps)\n",
    "        #\n",
    "        pred_data = RC.predict(forecast_steps)\n",
    "        #\n",
    "        for j in range(forecast_steps):\n",
    "            loss_value += np.linalg.norm(test_data[spinup_steps+j] - pred_data[j])**2 \\\n",
    "                        * np.exp(- (j+1) / (forecast_steps) )\n",
    "    return loss_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42a8094c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HP list length = 36\n"
     ]
    }
   ],
   "source": [
    "#######################################################################\n",
    "# H Y P E R P A R A M E T E R S   L I S T \n",
    "#######################################################################\n",
    "N_list      = [400]\n",
    "rhoSR_list  = [0.2, 0.5, 0.8]\n",
    "rhoA_list   = [0.02]\n",
    "alpha_list  = [0.4, 0.6, 0.8]\n",
    "sigma_list  = [0.084]\n",
    "sigmab_list = [1.1, 1.3, 1.5, 1.7]\n",
    "beta_list   = [8.5e-8]\n",
    "\n",
    "cart_prod = [(a,b,c,d,e,f,g)\n",
    "             for a in N_list       \n",
    "             for b in rhoSR_list   \n",
    "             for c in rhoA_list    \n",
    "             for d in alpha_list   \n",
    "             for e in sigma_list   \n",
    "             for f in sigmab_list  \n",
    "             for g in beta_list   ]\n",
    "\n",
    "len_vec = len(cart_prod)\n",
    "vec_loss_macro = np.zeros(len_vec)\n",
    "\n",
    "print(\"HP list length =\",len_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae59a5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "342\n"
     ]
    }
   ],
   "source": [
    "#######################################################################\n",
    "# L I S T   O F   R H O   V A L U E S\n",
    "#######################################################################\n",
    "rho_list  = np.arange(0, 331, 1)\n",
    "\n",
    "rho_list  = np.append(rho_list, np.array([13.926667, 23.9, 24.058, 470./19., 30.485]))\n",
    "rho_list  = np.append(rho_list, np.array([99.524, 100.795]))\n",
    "rho_list  = np.append(rho_list, np.array([148.4, 166.07, 214.364, 233.5]))\n",
    "\n",
    "rho_list  = np.sort(rho_list)\n",
    "\n",
    "len_rho_list = len(rho_list)\n",
    "\n",
    "print(len_rho_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b113a7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====   240) : Lorenz rho = 230.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.8\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     161993.94219355306\n",
      "=====   241) : Lorenz rho = 231.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.8\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     34339.44162297253\n",
      "=====   242) : Lorenz rho = 232.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     43795.88400486316\n",
      "=====   243) : Lorenz rho = 233.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.8\n",
      "            rhoA     0.02\n",
      "            alpha    0.8\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     70540.68717248437\n",
      "=====   244) : Lorenz rho = 233.500\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     277673.20873383025\n",
      "=====   245) : Lorenz rho = 234.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.8\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     13923.446449341025\n",
      "=====   246) : Lorenz rho = 235.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.8\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     68097.4025262058\n",
      "=====   247) : Lorenz rho = 236.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.8\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     8140553.8778817365\n",
      "=====   248) : Lorenz rho = 237.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     314763.0887438787\n",
      "=====   249) : Lorenz rho = 238.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.8\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     5058.633382107695\n",
      "=====   250) : Lorenz rho = 239.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     393361.0825917643\n",
      "=====   251) : Lorenz rho = 240.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.8\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     127275.06008052082\n",
      "=====   252) : Lorenz rho = 241.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     356820.7629775993\n",
      "=====   253) : Lorenz rho = 242.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     12634.35232481249\n",
      "=====   254) : Lorenz rho = 243.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     2970.399885093447\n",
      "=====   255) : Lorenz rho = 244.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     229802.77445427555\n",
      "=====   256) : Lorenz rho = 245.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     171634.96764055782\n",
      "=====   257) : Lorenz rho = 246.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     725372.5065124013\n",
      "=====   258) : Lorenz rho = 247.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.8\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     46351.680981679936\n",
      "=====   259) : Lorenz rho = 248.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.8\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     23157.543668527353\n",
      "=====   260) : Lorenz rho = 249.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     35618.27379200011\n",
      "=====   261) : Lorenz rho = 250.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.8\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     2643800.8049001475\n",
      "=====   262) : Lorenz rho = 251.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     152029.578863611\n",
      "=====   263) : Lorenz rho = 252.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.8\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     5244.826123716443\n",
      "=====   264) : Lorenz rho = 253.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.8\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     2321.136192273302\n",
      "=====   265) : Lorenz rho = 254.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     9318.320397017047\n",
      "=====   266) : Lorenz rho = 255.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.8\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     1005143.6305920639\n",
      "=====   267) : Lorenz rho = 256.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     15521.739690333257\n",
      "=====   268) : Lorenz rho = 257.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     830305.4243106213\n",
      "=====   269) : Lorenz rho = 258.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.8\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     161821.6695443799\n",
      "=====   270) : Lorenz rho = 259.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.8\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     35945.52147197595\n",
      "=====   271) : Lorenz rho = 260.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.8\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     645831.878657656\n",
      "=====   272) : Lorenz rho = 261.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     182665.70552136423\n",
      "=====   273) : Lorenz rho = 262.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     132068.37901447256\n",
      "=====   274) : Lorenz rho = 263.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     22714.47166386134\n",
      "=====   275) : Lorenz rho = 264.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.8\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     70979.34080286707\n",
      "=====   276) : Lorenz rho = 265.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     39645.121525531824\n",
      "=====   277) : Lorenz rho = 266.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     5571.959693549403\n",
      "=====   278) : Lorenz rho = 267.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.8\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     118327.11300968802\n",
      "=====   279) : Lorenz rho = 268.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     844.1114131409172\n",
      "=====   280) : Lorenz rho = 269.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.8\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     1861745.9430331432\n",
      "=====   281) : Lorenz rho = 270.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     1070.5962082663618\n",
      "=====   282) : Lorenz rho = 271.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     295079.0241445134\n",
      "=====   283) : Lorenz rho = 272.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     490196.1356829605\n",
      "=====   284) : Lorenz rho = 273.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     37444.11957218761\n",
      "=====   285) : Lorenz rho = 274.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     142242.9988730824\n",
      "=====   286) : Lorenz rho = 275.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     1484916.9546509124\n",
      "=====   287) : Lorenz rho = 276.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     24609.541984331805\n",
      "=====   288) : Lorenz rho = 277.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.8\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     38562.12991467288\n",
      "=====   289) : Lorenz rho = 278.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.1\n",
      "            beta     8.5e-08\n",
      "            loss     445078.3572670957\n",
      "=====   290) : Lorenz rho = 279.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     77164.89315892923\n",
      "=====   291) : Lorenz rho = 280.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.3\n",
      "            beta     8.5e-08\n",
      "            loss     346325.6189159038\n",
      "=====   292) : Lorenz rho = 281.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     98.80028259265664\n",
      "=====   293) : Lorenz rho = 282.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.8\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     2023004.5361912483\n",
      "=====   294) : Lorenz rho = 283.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     1493530.4548745947\n",
      "=====   295) : Lorenz rho = 284.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.8\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     3907591.44750985\n",
      "=====   296) : Lorenz rho = 285.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.6\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     335405.4761092204\n",
      "=====   297) : Lorenz rho = 286.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.5\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.7\n",
      "            beta     8.5e-08\n",
      "            loss     467431.19196359103\n",
      "=====   298) : Lorenz rho = 287.000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     263571.3809340703\n",
      "=====   299) : Lorenz rho = 288.000\n",
      "      ===== best hyperparameters\n",
      "            N        400\n",
      "            rhoSR    0.2\n",
      "            rhoA     0.02\n",
      "            alpha    0.4\n",
      "            sigma    0.084\n",
      "            sigmab   1.5\n",
      "            beta     8.5e-08\n",
      "            loss     1713264.292326691\n"
     ]
    }
   ],
   "source": [
    "#######################################################################\n",
    "# O P T I M I Z A T I O N   L O O P \n",
    "#######################################################################\n",
    "rnd_seed = 167\n",
    "nome_file = 'df_RC400_seed' + str(rnd_seed) + '_HP_parte05'\n",
    "s_from = 240\n",
    "s_to   = 300\n",
    "# store parameters and RC hyperparameters\n",
    "RC_HP   = np.zeros((s_to - s_from, 11)) #np.zeros((len_rho_list, 11))\n",
    "\n",
    "cnt = -1\n",
    "for s in range(s_from, s_to): #range(len_rho_list):\n",
    "    rho_Lorenz  = rho_list[s]\n",
    "    # read dataset\n",
    "    str_rho_Lorenz = (f'{rho_Lorenz:07.3f}').replace(\".\", \"_\")\n",
    "    print(\"===== \",f'{s:>4d}) : Lorenz rho = {rho_Lorenz:>7.3f}')\n",
    "    X = np.loadtxt('dataset/Lorenz_Dataset_'+str(str_rho_Lorenz)+'.csv',delimiter=\";\")\n",
    "    n_timesteps = len(X)\n",
    "    #print(n_timesteps)\n",
    "    \n",
    "    # splitting in training and test dataset\n",
    "    data_length          = len(X) \n",
    "    training_percentage  = .8\n",
    "    training_data_length = round(training_percentage * data_length) \n",
    "    #print(\"data_length          =\",data_length)\n",
    "    #print(\"training_data_length =\",training_data_length)\n",
    "    training_data = np.array(X[:training_data_length])\n",
    "    test_data     = np.array(X[training_data_length:])\n",
    "    \n",
    "    # Setting random seed for repeteability\n",
    "    \n",
    "    np.random.seed(rnd_seed)\n",
    "    random.seed(rnd_seed)\n",
    "    \n",
    "    # Set random forecast spinup list\n",
    "    M_forecasts = 100\n",
    "    S_forecast_steps = 500\n",
    "    max_spinup  = len(test_data) - S_forecast_steps\n",
    "    spinup_list = np.zeros(M_forecasts, dtype=np.uint32)\n",
    "    for i in range(M_forecasts):\n",
    "        spinup_list[i] = round(max_spinup * np.random.rand())    \n",
    "    \n",
    "    for i in range(len_vec):\n",
    "        # Setting random seed for repeteability\n",
    "        np.random.seed(rnd_seed)\n",
    "        random.seed(rnd_seed)\n",
    "        # set hyperparameter from cartesian product\n",
    "        D      = 3\n",
    "        N      = cart_prod[i][0]\n",
    "        rhoSR  = cart_prod[i][1]\n",
    "        rhoA   = cart_prod[i][2]\n",
    "        alpha  = cart_prod[i][3]  \n",
    "        sigma  = cart_prod[i][4]\n",
    "        sigmab = cart_prod[i][5] \n",
    "        beta   = cart_prod[i][6]\n",
    "        #\n",
    "        model   = ReservoirComputer(D, N, rhoSR, rhoA, alpha, sigma, sigmab, beta)\n",
    "        # training\n",
    "        washout = 100 # transitory skipped timesteps\n",
    "        model.train(training_data, washout)\n",
    "        # compute loss macro value\n",
    "        vec_loss_macro[i] = loss_macro(model,test_data,spinup_list,S_forecast_steps)\n",
    "        #print(\"i =\",i, vec_loss_macro[i])\n",
    "        \n",
    "        del model\n",
    "        \n",
    "    idx_vec_loss_macro = np.argmin(vec_loss_macro, axis=-1)\n",
    "    print(\"      ===== best hyperparameters\")\n",
    "    print(\"            N       \",cart_prod[idx_vec_loss_macro][0])\n",
    "    print(\"            rhoSR   \",cart_prod[idx_vec_loss_macro][1])\n",
    "    print(\"            rhoA    \",cart_prod[idx_vec_loss_macro][2])\n",
    "    print(\"            alpha   \",cart_prod[idx_vec_loss_macro][3])\n",
    "    print(\"            sigma   \",cart_prod[idx_vec_loss_macro][4])\n",
    "    print(\"            sigmab  \",cart_prod[idx_vec_loss_macro][5])\n",
    "    print(\"            beta    \",cart_prod[idx_vec_loss_macro][6])\n",
    "    print(\"            loss    \",vec_loss_macro[idx_vec_loss_macro])\n",
    "\n",
    "    # store best hyperparameters\n",
    "    cnt += 1\n",
    "    RC_HP[cnt][0]  = rho_Lorenz\n",
    "    RC_HP[cnt][1]  = D\n",
    "    RC_HP[cnt][2]  = washout\n",
    "    RC_HP[cnt][3]  = cart_prod[idx_vec_loss_macro][0]\n",
    "    RC_HP[cnt][4]  = cart_prod[idx_vec_loss_macro][1]\n",
    "    RC_HP[cnt][5]  = cart_prod[idx_vec_loss_macro][2]\n",
    "    RC_HP[cnt][6]  = cart_prod[idx_vec_loss_macro][3]\n",
    "    RC_HP[cnt][7]  = cart_prod[idx_vec_loss_macro][4]\n",
    "    RC_HP[cnt][8]  = cart_prod[idx_vec_loss_macro][5]\n",
    "    RC_HP[cnt][9]  = cart_prod[idx_vec_loss_macro][6]\n",
    "    RC_HP[cnt][10] = vec_loss_macro[idx_vec_loss_macro]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26b060c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy array to pandas DataFrame\n",
    "df_RC_HP = pd.DataFrame(RC_HP)\n",
    "\n",
    "# Name columns\n",
    "df_RC_HP.columns =['rho_Lorenz',\n",
    "                   'D',\n",
    "                   'washout',\n",
    "                   'N',        \n",
    "                   'rhoSR',   \n",
    "                   'rhoA',    \n",
    "                   'alpha',    \n",
    "                   'sigma',    \n",
    "                   'sigmab',   \n",
    "                   'beta',\n",
    "                   'loss']\n",
    "\n",
    "# Save DataFrame to .csv\n",
    "df_RC_HP.to_csv('climate/'+ nome_file + '.csv', index=False, header=True, decimal='.', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60fe6ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Elapsed time 4.07e+04 s\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------------------------------------------#---------------------------------------------------------------------#\n",
    "# Elapsed time\n",
    "#---------------------------------------------------------------------#\n",
    "print(f'\\nElapsed time {time.time() - start_time:6.2e} s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1fadde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
